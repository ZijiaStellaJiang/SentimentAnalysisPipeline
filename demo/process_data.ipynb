{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9d4a45de-e45a-4c68-a713-39ecc3d7ea68",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import pandas as pd\n",
    "from delta import DeltaTable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "85338311-1537-40e4-bcef-cc0eebacda56",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "file_path = \"dbfs:/FileStore/tables/IMDB_Dataset.csv\"\n",
    "delta_path = \"/mnt/delta/data\"\n",
    "\n",
    "# create a SparkSession\n",
    "spark = SparkSession.builder.appName(\"Parse data\").getOrCreate()\n",
    "\n",
    "# read the csv file, and convert to pyspark DataFrame\n",
    "df_pd = pd.read_csv(\"/dbfs/FileStore/tables/IMDB_Dataset.csv\", dtype=str)\n",
    "df_spark = spark.createDataFrame(df_pd)\n",
    "\n",
    "# write the DataFrame to Delta Lake\n",
    "df_spark.write.format('delta').mode('overwrite').save(delta_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "14a96e9c-8cf2-49a3-98e6-5492f477954d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------+\n|              review|sentiment|\n+--------------------+---------+\n|One of the other ...| positive|\n|A wonderful littl...| positive|\n|I thought this wa...| positive|\n|Basically there's...| negative|\n|Petter Mattei's \"...| positive|\n|Probably my all-t...| positive|\n|I sure would like...| positive|\n|This show was an ...| negative|\n|Encouraged by the...| negative|\n|If you like origi...| positive|\n|Phil the Alien is...| negative|\n|I saw this movie ...| negative|\n|So im not a big f...| negative|\n|The cast played S...| negative|\n|This a fantastic ...| positive|\n|Kind of drawn in ...| negative|\n|Some films just s...| positive|\n|This movie made i...| negative|\n|I remember this f...| positive|\n|An awful film! It...| negative|\n+--------------------+---------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "# check whether we can correctly access data from delta lake table\n",
    "\n",
    "# Read the Delta table into a PySpark DataFrame\n",
    "delta_table = spark.read.format(\"delta\").load(delta_path)\n",
    "delta_table.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "98b27805-8477-44f1-97f4-818bb405c0a1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "process_data",
   "notebookOrigID": 3746746332029758,
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
